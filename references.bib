

%trick for sorting with van and von
@preamble{"\newcommand{\SortNoop}[1]{}"}




@article{Wimmer2004,
author = {Wimmer, Michael and Scherzer, Daniel and Purgathofer, Werner},
doi = {10.2312/EGWR/EGSR04/143-151},
file = {:home/heider/Downloads/shadows{\_}egsr2004{\_}revised.pdf:pdf},
isbn = {3-905673-12-6},
journal = {Rendering Techniques 2004 (Proceedings Eurographics Symposium on Rendering)},
keywords = {real-time rendering,shadows},
pages = {143--151},
title = {{Light Space Perspective Shadow Maps}},
url = {http://www.cg.tuwien.ac.at/research/publications/2004/Wimmer-2004-LSPM/},
year = {2004}
}

@misc{unity3d,
author = {Unity},
year = {2017},
abstract = {Unity is the ultimate game development platform. Use Unity to build high-quality 3D and 2D games, deploy them across mobile, desktop, VR/AR, consoles or the Web, and connect with loyal and enthusiastic players and customers.},
title = {{Unity - Game Engine}},
url = {https://unity3d.com},
urldate = {2017-08-15 21:24:49},
note = "Available at \url{https://unity3d.com}"
}


@misc{unityshadowmapsize,
author = {Unity},
year = {2017},
title = {{Unity - Manual: Shadow Size Computation}},
url = {https://docs.unity3d.com/460/Documentation/Manual/ShadowSizeDetails.html},
urldate = {2017-07-17},
note = "Available at \url{https://docs.unity3d.com/460/Documentation/Manual/ShadowSizeDetails.html}"
}

@misc{unitylighttypes,
author = {Unity},
year = {2017},
title = {{Unity - Types of light}},
url = {https://docs.unity3d.com/Manual/Lighting.html},
urldate = {2017-08-16 19:11:53},
note = "Available at \url{https://docs.unity3d.com/Manual/Lighting.html}"
}

@misc{unityphysics,
author = {Unity},
year = {2017},
title = {{Unity - Manual: Physics}},
url = {https://docs.unity3d.com/Manual/PhysicsSection.html},
urldate = {2017-08-17 09:13:08},
note = "Available at \url{https://docs.unity3d.com/Manual/PhysicsSection.html}"
}

@misc{applerefiner,
author = {Apple},
year = {2017},
title = {{Improving the Realism of Synthetic Images - Apple}},
url = {https://machinelearning.apple.com/2017/07/07/GAN.html},
urldate = {2017-08-17 09:24:49},
note = "Available at \url{https://machinelearning.apple.com/2017/07/07/GAN.html}"
}

@article{Dyrstad2016,
abstract = {The focus of this project has been on training convolutional neural networks for grasp detection with synthetic data. Convolutional neural networks have had great success on a wide variety of computer vision tasks, but they require large amounts of labelled training data, which currently is non existent for grasp detection tasks. In this thesis, a novel approach for generating large amounts of synthetic data for grasp detection is proposed. By working solely with depth images, realistic looking data can be generated with 3D models in a virtual environment. It is proposed to use simulated physics to ensure that the generated depth images captures objects in natural poses. Additionally, the use of heuristics for choosing the best grip vectors for the objects in relation to their environment is proposed, to serve as the labels for the generated depth images. A virtual environment for synthetic depth image generation was created and a convolutional neural network was trained on the generated data. The results show that neural networks can ﬁnd good grasps from the synthetic depth images for three diﬀerent types of objects in cluttered scenes. A novel way of creating real world data sets for grasping using a head mounted display and tracked hand controllers is also proposed. The results show that this may enable easy and fast labelling of real data which can be performed without training by non-technical peopl},
author = {Dyrstad, Jonatan Sj{\o}lund},
pages = {84},
title = {{Training convolutional neural networks in virtual reality for grasp detection from 3D images}},
year = {2016}
}


@article{Shotton2013,
abstract = {We describe two new approaches to human pose estimation. Both can quickly and accurately predict the 3D positions of body joints from a single depth image without using any temporal information. The key to both approaches is the use of a large, realistic, and highly varied synthetic set of training images. This allows us to learn models that are largely invariant to factors such as pose, body shape, field-of-view cropping, and clothing. Our first approach employs an intermediate body parts representation, designed so that an accurate per-pixel classification of the parts will localize the joints of the body. The second approach instead directly regresses the positions of body joints. By using simple depth pixel comparison features and parallelizable decision forests, both approaches can run super-real time on consumer hardware. Our evaluation investigates many aspects of our methods, and compares the approaches to each other and to the state of the art. Results on silhouettes suggest broader applicability to other imaging modalities.},
author = {Shotton, Jamie and Girshick, Ross and Fitzgibbon, Andrew and Sharp, Toby and Cook, Mat and Finocchio, Mark and Moore, Richard and Kohli, Pushmeet and Criminisi, Antonio and Kipman, Alex and Blake, Andrew},
doi = {10.1109/TPAMI.2012.241},
file = {:home/heider/Downloads/main-39.pdf:pdf},
isbn = {978-1-4471-4928-6},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer vision,depth cues,games,machine learning,pixel classification,range data},
number = {12},
pages = {2821--2840},
pmid = {24136424},
title = {{Efficient human pose estimation from single depth images}},
volume = {35},
year = {2013}
}

@misc{openai,
author = {OpenAI},
year = {2017},
abstract = {Train your agent using our open-source library, and then upload and compare your results.},
shorttitle = {OpenAI Gym},
title = {{OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms}},
url = {https://gym.openai.com},
note = "Available at \url{https://gym.openai.com}"
}


@article{Tobin2017,
abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to {\$}1.5{\$}cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
archivePrefix = {arXiv},
arxivId = {1703.06907},
author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
eprint = {1703.06907},
file = {:home/heider/Downloads/1703.06907.pdf:pdf},
journal = {arXiv},
title = {{Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World}},
url = {http://arxiv.org/abs/1703.06907},
year = {2017}
}
